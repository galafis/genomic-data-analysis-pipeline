# üáßüá∑ Pipeline de An√°lise de Dados Gen√¥micos | üá∫üá∏ Genomic Data Analysis Pipeline

<div align="center">

![R](https://img.shields.io/badge/R-276DC3?style=for-the-badge&logo=r&logoColor=white)
![Python](https://img.shields.io/badge/Python-3776AB?style=for-the-badge&logo=python&logoColor=white)
![Bioconductor](https://img.shields.io/badge/Bioconductor-1f65cc?style=for-the-badge&logo=r&logoColor=white)
![Nextflow](https://img.shields.io/badge/Nextflow-0dc09d?style=for-the-badge&logo=nextflow&logoColor=white)
![Docker](https://img.shields.io/badge/Docker-2496ED?style=for-the-badge&logo=docker&logoColor=white)
![AWS](https://img.shields.io/badge/AWS-232F3E?style=for-the-badge&logo=amazon-aws&logoColor=white)
![Snakemake](https://img.shields.io/badge/Snakemake-4B8BBE?style=for-the-badge&logo=python&logoColor=white)

**Pipeline enterprise de bioinform√°tica para an√°lise de dados gen√¥micos de larga escala com processamento distribu√≠do, machine learning e visualiza√ß√µes interativas**

[üß¨ Genomics](#-an√°lise-gen√¥mica) ‚Ä¢ [‚ö° HPC](#-computa√ß√£o-de-alta-performance) ‚Ä¢ [ü§ñ ML](#-machine-learning-gen√¥mico) ‚Ä¢ [üìä Visualization](#-visualiza√ß√µes-interativas)

</div>

---

## üáßüá∑ Portugu√™s

### üéØ Vis√£o Geral

Pipeline **enterprise-grade** de bioinform√°tica que processa dados gen√¥micos de larga escala para descoberta de biomarcadores, an√°lise de variantes e medicina personalizada:

- üß¨ **An√°lise Multi-√îmica**: DNA-seq, RNA-seq, ChIP-seq, ATAC-seq, Single-cell
- ‚ö° **Processamento Distribu√≠do**: Nextflow + Snakemake + AWS Batch
- ü§ñ **Machine Learning**: Predi√ß√£o de fen√≥tipos, classifica√ß√£o de variantes
- üìä **Visualiza√ß√µes Avan√ßadas**: Shiny + Plotly + IGV.js
- üî¨ **An√°lises Estat√≠sticas**: GWAS, eQTL, pathway analysis
- üè• **Aplica√ß√µes Cl√≠nicas**: Diagn√≥stico molecular, farmacogen√¥mica

### üèÜ Objetivos do Pipeline

- **Processar genomas** completos (WGS/WES) em <4 horas
- **Identificar variantes** com precis√£o >99.5%
- **Descobrir biomarcadores** com signific√¢ncia estat√≠stica
- **Predizer fen√≥tipos** com acur√°cia >85%
- **Gerar relat√≥rios** cl√≠nicos automatizados

### üõ†Ô∏è Stack Tecnol√≥gico Avan√ßado

#### Bioinform√°tica & Genomics
- **R 4.3+**: Linguagem principal para an√°lises estat√≠sticas
- **Bioconductor**: Pacotes especializados em bioinform√°tica
- **Python 3.9+**: Scripts de processamento e ML
- **BWA-MEM2**: Alinhamento de sequ√™ncias otimizado
- **GATK 4**: Variant calling e processamento
- **VEP**: Anota√ß√£o de variantes
- **PLINK 2**: An√°lises de gen√©tica populacional

#### Workflow Management
- **Nextflow**: Workflow engine para pipelines escal√°veis
- **Snakemake**: Workflow management em Python
- **Cromwell**: Execution engine para WDL
- **Docker**: Containeriza√ß√£o de ferramentas
- **Singularity**: Containers para HPC
- **Conda**: Gerenciamento de ambientes

#### High-Performance Computing
- **AWS Batch**: Processamento distribu√≠do na nuvem
- **Slurm**: Job scheduler para clusters HPC
- **GNU Parallel**: Paraleliza√ß√£o de tarefas
- **OpenMP**: Programa√ß√£o paralela
- **MPI**: Message passing interface
- **CUDA**: GPU computing para an√°lises intensivas

#### Machine Learning & Statistics
- **scikit-learn**: Algoritmos de ML cl√°ssicos
- **TensorFlow**: Deep learning para genomics
- **XGBoost**: Gradient boosting para predi√ß√µes
- **PLINK**: An√°lises de associa√ß√£o gen√¥mica
- **R/qtl2**: QTL mapping
- **GCTA**: An√°lises de herdabilidade

#### Data Storage & Management
- **HDF5**: Armazenamento eficiente de dados
- **Parquet**: Formato columnar para big data
- **MongoDB**: Database para metadados
- **MinIO**: Object storage compat√≠vel S3
- **DuckDB**: OLAP database para an√°lises
- **Apache Arrow**: Processamento in-memory

#### Visualization & Reporting
- **R Shiny**: Dashboards interativos
- **Plotly**: Visualiza√ß√µes interativas
- **IGV.js**: Genome browser web
- **Circos**: Visualiza√ß√µes circulares
- **ggplot2**: Gr√°ficos estat√≠sticos
- **R Markdown**: Relat√≥rios reproduz√≠veis

### üìã Arquitetura do Pipeline

```
genomic-data-analysis-pipeline/
‚îú‚îÄ‚îÄ üìÅ workflows/                     # Workflows de an√°lise
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ nextflow/                  # Pipelines Nextflow
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ main.nf                # Pipeline principal
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ modules/               # M√≥dulos reutiliz√°veis
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ quality_control.nf # Controle de qualidade
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ alignment.nf       # Alinhamento
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ variant_calling.nf # Chamada de variantes
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ annotation.nf      # Anota√ß√£o
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÑ analysis.nf        # An√°lises downstream
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ conf/                  # Configura√ß√µes
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ base.config        # Configura√ß√£o base
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ aws.config         # Configura√ß√£o AWS
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ slurm.config       # Configura√ß√£o Slurm
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÑ docker.config      # Configura√ß√£o Docker
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÑ bin/                   # Scripts auxiliares
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ snakemake/                 # Pipelines Snakemake
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ Snakefile              # Workflow principal
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ rules/                 # Regras do workflow
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ preprocessing.smk  # Pr√©-processamento
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ alignment.smk      # Alinhamento
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ variant_calling.smk # Chamada variantes
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ annotation.smk     # Anota√ß√£o
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÑ analysis.smk       # An√°lises
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ envs/                  # Ambientes conda
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ alignment.yaml     # Ambiente alinhamento
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ variant_calling.yaml # Ambiente variantes
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÑ analysis.yaml      # Ambiente an√°lises
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÑ scripts/               # Scripts Python/R
‚îÇ   ‚îî‚îÄ‚îÄ üìÅ cwl/                       # Common Workflow Language
‚îÇ       ‚îú‚îÄ‚îÄ üìÑ main.cwl               # Workflow principal CWL
‚îÇ       ‚îú‚îÄ‚îÄ üìÑ tools/                 # Ferramentas CWL
‚îÇ       ‚îî‚îÄ‚îÄ üìÑ workflows/             # Sub-workflows
‚îú‚îÄ‚îÄ üìÅ src/                           # C√≥digo fonte
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ r/                         # Scripts R
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÅ quality_control/       # Controle qualidade
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ fastqc_analysis.R  # An√°lise FastQC
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ multiqc_parser.R   # Parser MultiQC
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÑ contamination_check.R # Verifica√ß√£o contamina√ß√£o
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÅ variant_analysis/      # An√°lise variantes
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ variant_filtering.R # Filtragem variantes
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ annotation_parser.R # Parser anota√ß√µes
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ pathogenicity_prediction.R # Predi√ß√£o patogenicidade
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÑ population_genetics.R # Gen√©tica populacional
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÅ gwas/                  # Genome-wide association
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ gwas_analysis.R    # An√°lise GWAS
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ manhattan_plot.R   # Manhattan plots
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ qq_plot.R          # Q-Q plots
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÑ ld_analysis.R      # Linkage disequilibrium
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÅ expression/            # An√°lise express√£o
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ differential_expression.R # Express√£o diferencial
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ pathway_analysis.R # An√°lise pathways
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ gene_set_enrichment.R # Enriquecimento
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÑ coexpression_networks.R # Redes coexpress√£o
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÅ single_cell/           # Single-cell analysis
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ seurat_pipeline.R  # Pipeline Seurat
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ cell_type_annotation.R # Anota√ß√£o tipos celulares
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ trajectory_analysis.R # An√°lise trajet√≥rias
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÑ integration_analysis.R # Integra√ß√£o datasets
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÅ visualization/         # Visualiza√ß√µes
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ genomic_plots.R    # Plots gen√¥micos
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ heatmaps.R         # Heatmaps
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ circos_plots.R     # Plots Circos
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÑ interactive_plots.R # Plots interativos
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÅ utils/                 # Utilit√°rios R
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ üìÑ data_loading.R     # Carregamento dados
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ üìÑ statistical_tests.R # Testes estat√≠sticos
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ üìÑ file_parsers.R     # Parsers arquivos
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ üìÑ helper_functions.R # Fun√ß√µes auxiliares
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ python/                    # Scripts Python
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÅ preprocessing/         # Pr√©-processamento
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ __init__.py        # Inicializa√ß√£o
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ fastq_processor.py # Processador FASTQ
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ quality_filter.py  # Filtro qualidade
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÑ adapter_trimmer.py # Trimmer adaptadores
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÅ alignment/             # Alinhamento
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ __init__.py        # Inicializa√ß√£o
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ bwa_wrapper.py     # Wrapper BWA
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ sam_processor.py   # Processador SAM/BAM
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÑ alignment_stats.py # Estat√≠sticas alinhamento
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÅ variant_calling/       # Chamada variantes
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ __init__.py        # Inicializa√ß√£o
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ gatk_wrapper.py    # Wrapper GATK
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ vcf_processor.py   # Processador VCF
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÑ variant_filter.py  # Filtro variantes
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÅ annotation/            # Anota√ß√£o
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ __init__.py        # Inicializa√ß√£o
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ vep_wrapper.py     # Wrapper VEP
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ annovar_wrapper.py # Wrapper ANNOVAR
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÑ annotation_parser.py # Parser anota√ß√µes
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÅ machine_learning/      # Machine Learning
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ __init__.py        # Inicializa√ß√£o
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ feature_engineering.py # Feature engineering
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ phenotype_predictor.py # Preditor fen√≥tipos
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ variant_classifier.py # Classificador variantes
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ drug_response_predictor.py # Preditor resposta drogas
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÑ ensemble_models.py # Modelos ensemble
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÅ population_genetics/   # Gen√©tica populacional
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ __init__.py        # Inicializa√ß√£o
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ pca_analysis.py    # An√°lise PCA
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ admixture_analysis.py # An√°lise ADMIXTURE
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ fst_calculator.py  # Calculadora FST
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÑ demographic_inference.py # Infer√™ncia demogr√°fica
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÅ utils/                 # Utilit√°rios Python
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ __init__.py        # Inicializa√ß√£o
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ file_handlers.py   # Manipuladores arquivos
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ data_validators.py # Validadores dados
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ config_parser.py   # Parser configura√ß√µes
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÑ logger.py          # Sistema logging
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÅ visualization/         # Visualiza√ß√µes Python
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ üìÑ __init__.py        # Inicializa√ß√£o
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ üìÑ plotly_genomics.py # Plots Plotly gen√¥micos
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ üìÑ matplotlib_plots.py # Plots Matplotlib
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ üìÑ bokeh_interactive.py # Plots Bokeh interativos
‚îÇ   ‚îî‚îÄ‚îÄ üìÅ bash/                      # Scripts Bash
‚îÇ       ‚îú‚îÄ‚îÄ üìÑ setup_environment.sh   # Setup ambiente
‚îÇ       ‚îú‚îÄ‚îÄ üìÑ download_references.sh # Download refer√™ncias
‚îÇ       ‚îú‚îÄ‚îÄ üìÑ run_pipeline.sh        # Execu√ß√£o pipeline
‚îÇ       ‚îî‚îÄ‚îÄ üìÑ cleanup.sh             # Limpeza arquivos
‚îú‚îÄ‚îÄ üìÅ shiny_apps/                    # Aplica√ß√µes Shiny
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ genomic_browser/           # Browser gen√¥mico
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ app.R                  # Aplica√ß√£o principal
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ ui.R                   # Interface usu√°rio
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ server.R               # L√≥gica servidor
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ global.R               # Configura√ß√µes globais
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÑ www/                   # Recursos web
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ variant_explorer/          # Explorador variantes
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ app.R                  # Aplica√ß√£o principal
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ modules/               # M√≥dulos Shiny
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ variant_table.R    # Tabela variantes
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ annotation_panel.R # Painel anota√ß√µes
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÑ frequency_plots.R  # Plots frequ√™ncia
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÑ data/                  # Dados aplica√ß√£o
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ gwas_results/              # Resultados GWAS
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ app.R                  # Aplica√ß√£o principal
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ manhattan_module.R     # M√≥dulo Manhattan
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ qq_module.R            # M√≥dulo Q-Q
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÑ locus_zoom_module.R    # M√≥dulo LocusZoom
‚îÇ   ‚îî‚îÄ‚îÄ üìÅ expression_dashboard/      # Dashboard express√£o
‚îÇ       ‚îú‚îÄ‚îÄ üìÑ app.R                  # Aplica√ß√£o principal
‚îÇ       ‚îú‚îÄ‚îÄ üìÑ differential_module.R  # M√≥dulo express√£o diferencial
‚îÇ       ‚îú‚îÄ‚îÄ üìÑ pathway_module.R       # M√≥dulo pathways
‚îÇ       ‚îî‚îÄ‚îÄ üìÑ network_module.R       # M√≥dulo redes
‚îú‚îÄ‚îÄ üìÅ data/                          # Dados e refer√™ncias
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ reference/                 # Genomas refer√™ncia
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ GRCh38/                # Genoma humano GRCh38
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ genome.fa          # Sequ√™ncia gen√¥mica
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ genome.fa.fai      # √çndice FASTA
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÑ annotations.gtf    # Anota√ß√µes genes
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ dbSNP/                 # Database SNPs
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ ClinVar/               # Variantes cl√≠nicas
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÑ gnomAD/                # Frequ√™ncias populacionais
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ test_data/                 # Dados teste
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ sample_fastq/          # Arquivos FASTQ exemplo
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ sample_vcf/            # Arquivos VCF exemplo
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÑ sample_phenotypes.txt  # Fen√≥tipos exemplo
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ databases/                 # Databases anota√ß√£o
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ OMIM/                  # Online Mendelian Inheritance
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ HGMD/                  # Human Gene Mutation Database
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ PharmGKB/              # Pharmacogenomics database
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÑ GTEx/                  # Gene expression database
‚îÇ   ‚îî‚îÄ‚îÄ üìÅ results/                   # Resultados an√°lises
‚îÇ       ‚îú‚îÄ‚îÄ üìÑ quality_control/       # QC results
‚îÇ       ‚îú‚îÄ‚îÄ üìÑ variant_calling/       # Variantes chamadas
‚îÇ       ‚îú‚îÄ‚îÄ üìÑ annotation/            # Anota√ß√µes
‚îÇ       ‚îî‚îÄ‚îÄ üìÑ analysis/              # An√°lises downstream
‚îú‚îÄ‚îÄ üìÅ notebooks/                     # Jupyter/R notebooks
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ 01_data_exploration.Rmd    # Explora√ß√£o dados
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ 02_quality_control.ipynb   # Controle qualidade
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ 03_variant_analysis.Rmd    # An√°lise variantes
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ 04_gwas_analysis.Rmd       # An√°lise GWAS
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ 05_expression_analysis.Rmd # An√°lise express√£o
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ 06_single_cell_analysis.Rmd # An√°lise single-cell
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ 07_machine_learning.ipynb  # Machine learning
‚îÇ   ‚îî‚îÄ‚îÄ üìÑ 08_pharmacogenomics.Rmd    # Farmacogen√¥mica
‚îú‚îÄ‚îÄ üìÅ config/                        # Configura√ß√µes
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ pipeline_config.yaml       # Configura√ß√£o pipeline
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ reference_config.yaml      # Configura√ß√£o refer√™ncias
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ cluster_config.yaml        # Configura√ß√£o cluster
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ aws_config.yaml            # Configura√ß√£o AWS
‚îÇ   ‚îî‚îÄ‚îÄ üìÑ tool_versions.yaml         # Vers√µes ferramentas
‚îú‚îÄ‚îÄ üìÅ containers/                    # Containers Docker
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ Dockerfile.base            # Container base
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ Dockerfile.alignment       # Container alinhamento
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ Dockerfile.variant_calling # Container variant calling
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ Dockerfile.annotation      # Container anota√ß√£o
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ Dockerfile.analysis        # Container an√°lises
‚îÇ   ‚îî‚îÄ‚îÄ üìÑ Dockerfile.shiny           # Container Shiny apps
‚îú‚îÄ‚îÄ üìÅ tests/                         # Testes automatizados
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ unit/                      # Testes unit√°rios
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ test_preprocessing.py  # Teste pr√©-processamento
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ test_variant_calling.py # Teste variant calling
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÑ test_annotation.py     # Teste anota√ß√£o
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ integration/               # Testes integra√ß√£o
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ test_full_pipeline.py  # Teste pipeline completo
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÑ test_workflows.py      # Teste workflows
‚îÇ   ‚îî‚îÄ‚îÄ üìÅ data/                      # Dados para testes
‚îú‚îÄ‚îÄ üìÅ docs/                          # Documenta√ß√£o
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ README.md                  # Este arquivo
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ INSTALLATION.md            # Guia instala√ß√£o
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ USAGE.md                   # Guia uso
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ API_REFERENCE.md           # Refer√™ncia API
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ WORKFLOWS.md               # Documenta√ß√£o workflows
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ TROUBLESHOOTING.md         # Solu√ß√£o problemas
‚îÇ   ‚îî‚îÄ‚îÄ üìÅ images/                    # Imagens documenta√ß√£o
‚îú‚îÄ‚îÄ üìÅ deployment/                    # Deployment
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ aws/                       # Deployment AWS
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ cloudformation.yaml    # Template CloudFormation
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ batch_job_definition.json # Defini√ß√£o job Batch
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÑ lambda_functions/      # Fun√ß√µes Lambda
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ kubernetes/                # Deployment Kubernetes
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ namespace.yaml         # Namespace
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ pipeline-deployment.yaml # Deployment pipeline
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÑ shiny-deployment.yaml  # Deployment Shiny
‚îÇ   ‚îî‚îÄ‚îÄ üìÅ slurm/                     # Scripts Slurm
‚îÇ       ‚îú‚îÄ‚îÄ üìÑ submit_pipeline.sh     # Submit pipeline
‚îÇ       ‚îî‚îÄ‚îÄ üìÑ job_templates/         # Templates jobs
‚îú‚îÄ‚îÄ üìÑ requirements.txt               # Depend√™ncias Python
‚îú‚îÄ‚îÄ üìÑ requirements-r.txt             # Depend√™ncias R
‚îú‚îÄ‚îÄ üìÑ environment.yml                # Ambiente Conda
‚îú‚îÄ‚îÄ üìÑ .gitignore                     # Arquivos ignorados
‚îú‚îÄ‚îÄ üìÑ LICENSE                        # Licen√ßa MIT
‚îú‚îÄ‚îÄ üìÑ Makefile                       # Comandos make
‚îú‚îÄ‚îÄ üìÑ nextflow.config                # Configura√ß√£o Nextflow
‚îú‚îÄ‚îÄ üìÑ docker-compose.yml             # Docker compose
‚îî‚îÄ‚îÄ üìÑ .github/                       # GitHub workflows
    ‚îî‚îÄ‚îÄ üìÑ workflows/                 # CI/CD workflows
        ‚îú‚îÄ‚îÄ üìÑ ci.yml                 # Continuous Integration
        ‚îú‚îÄ‚îÄ üìÑ test-pipeline.yml      # Teste pipeline
        ‚îî‚îÄ‚îÄ üìÑ deploy.yml             # Deploy autom√°tico
```

### üß¨ An√°lise Gen√¥mica

#### 1. üî¨ Pipeline de Variant Calling

**Workflow Nextflow Otimizado**
```nextflow
#!/usr/bin/env nextflow

/*
 * Genomic Data Analysis Pipeline
 * High-performance variant calling workflow
 */

nextflow.enable.dsl = 2

// Parameters
params.input_dir = "data/fastq"
params.output_dir = "results"
params.reference = "data/reference/GRCh38/genome.fa"
params.known_sites = "data/reference/dbSNP/dbsnp.vcf.gz"
params.intervals = "data/reference/intervals/exome.bed"
params.cpu = 8
params.memory = "32.GB"

// Include modules
include { FASTQC } from './modules/quality_control'
include { BWA_MEM } from './modules/alignment'
include { GATK_HAPLOTYPECALLER } from './modules/variant_calling'
include { VEP_ANNOTATION } from './modules/annotation'

// Main workflow
workflow {
    // Input channel
    fastq_ch = Channel
        .fromFilePairs("${params.input_dir}/*_{R1,R2}.fastq.gz")
        .map { sample_id, reads -> 
            tuple(sample_id, reads[0], reads[1])
        }
    
    // Quality control
    fastqc_results = FASTQC(fastq_ch)
    
    // Alignment
    aligned_bams = BWA_MEM(
        fastq_ch,
        params.reference
    )
    
    // Variant calling
    variants = GATK_HAPLOTYPECALLER(
        aligned_bams,
        params.reference,
        params.known_sites,
        params.intervals
    )
    
    // Annotation
    annotated_variants = VEP_ANNOTATION(
        variants,
        params.reference
    )
    
    // Emit results
    annotated_variants.view()
}

// BWA-MEM alignment process
process BWA_MEM {
    tag "$sample_id"
    cpus params.cpu
    memory params.memory
    
    publishDir "${params.output_dir}/alignment", mode: 'copy'
    
    input:
    tuple val(sample_id), path(read1), path(read2)
    path reference
    
    output:
    tuple val(sample_id), path("${sample_id}.sorted.bam"), path("${sample_id}.sorted.bam.bai")
    
    script:
    """
    # Create read group
    RG="@RG\\tID:${sample_id}\\tSM:${sample_id}\\tPL:ILLUMINA\\tLB:${sample_id}"
    
    # Align with BWA-MEM
    bwa mem -t ${task.cpus} -R "\$RG" \\
        ${reference} ${read1} ${read2} | \\
        samtools sort -@ ${task.cpus} -o ${sample_id}.sorted.bam -
    
    # Index BAM
    samtools index ${sample_id}.sorted.bam
    
    # Generate alignment statistics
    samtools flagstat ${sample_id}.sorted.bam > ${sample_id}.flagstat
    samtools stats ${sample_id}.sorted.bam > ${sample_id}.stats
    """
}

// GATK HaplotypeCaller process
process GATK_HAPLOTYPECALLER {
    tag "$sample_id"
    cpus params.cpu
    memory params.memory
    
    publishDir "${params.output_dir}/variants", mode: 'copy'
    
    input:
    tuple val(sample_id), path(bam), path(bai)
    path reference
    path known_sites
    path intervals
    
    output:
    tuple val(sample_id), path("${sample_id}.g.vcf.gz"), path("${sample_id}.g.vcf.gz.tbi")
    
    script:
    """
    # Call variants with GATK HaplotypeCaller
    gatk HaplotypeCaller \\
        -R ${reference} \\
        -I ${bam} \\
        -O ${sample_id}.g.vcf.gz \\
        -L ${intervals} \\
        --dbsnp ${known_sites} \\
        --emit-ref-confidence GVCF \\
        --annotation-group StandardAnnotation \\
        --annotation-group AS_StandardAnnotation \\
        --native-pair-hmm-threads ${task.cpus}
    
    # Index GVCF
    gatk IndexFeatureFile -I ${sample_id}.g.vcf.gz
    """
}
```

#### 2. üìä An√°lise Estat√≠stica Avan√ßada em R

**GWAS Analysis com Controle de Popula√ß√£o**
```r
# Advanced GWAS Analysis Pipeline
# Population structure control and association testing

library(data.table)
library(PLINK)
library(qqman)
library(CMplot)
library(genetics)
library(SNPRelate)
library(gdsfmt)
library(parallel)

#' Comprehensive GWAS Analysis Class
#' 
#' Performs genome-wide association studies with population structure control,
#' multiple testing correction, and advanced visualization
GWASAnalyzer <- R6Class("GWASAnalyzer",
  public = list(
    
    #' Initialize GWAS analyzer
    #' @param plink_prefix Prefix for PLINK binary files
    #' @param phenotype_file Path to phenotype file
    #' @param covariate_file Path to covariate file (optional)
    initialize = function(plink_prefix, phenotype_file, covariate_file = NULL) {
      private$plink_prefix <- plink_prefix
      private$phenotype_file <- phenotype_file
      private$covariate_file <- covariate_file
      
      # Load data
      private$load_genotype_data()
      private$load_phenotype_data()
      if (!is.null(covariate_file)) {
        private$load_covariate_data()
      }
      
      message("GWAS Analyzer initialized successfully")
    },
    
    #' Perform quality control on genotype data
    #' @param maf_threshold Minor allele frequency threshold (default: 0.01)
    #' @param geno_threshold Genotyping rate threshold (default: 0.95)
    #' @param hwe_threshold Hardy-Weinberg equilibrium p-value threshold (default: 1e-6)
    #' @param mind_threshold Individual missingness threshold (default: 0.1)
    perform_qc = function(maf_threshold = 0.01, geno_threshold = 0.95, 
                         hwe_threshold = 1e-6, mind_threshold = 0.1) {
      
      message("Performing genotype quality control...")
      
      # Create QC command
      qc_command <- sprintf(
        "plink --bfile %s --maf %f --geno %f --hwe %f --mind %f --make-bed --out %s_qc",
        private$plink_prefix, maf_threshold, 1 - geno_threshold, 
        hwe_threshold, mind_threshold, private$plink_prefix
      )
      
      # Execute QC
      system(qc_command)
      
      # Update prefix to QC'd data
      private$plink_prefix <- paste0(private$plink_prefix, "_qc")
      
      # Generate QC report
      private$generate_qc_report(maf_threshold, geno_threshold, hwe_threshold, mind_threshold)
      
      message("Quality control completed")
    },
    
    #' Calculate population structure using PCA
    #' @param n_pcs Number of principal components to calculate (default: 10)
    #' @param ld_window_size LD pruning window size (default: 50)
    #' @param ld_step_size LD pruning step size (default: 5)
    #' @param ld_r2_threshold LD r¬≤ threshold (default: 0.2)
    calculate_population_structure = function(n_pcs = 10, ld_window_size = 50, 
                                            ld_step_size = 5, ld_r2_threshold = 0.2) {
      
      message("Calculating population structure...")
      
      # LD pruning for PCA
      ld_prune_command <- sprintf(
        "plink --bfile %s --indep-pairwise %d %d %f --out %s_ld_pruned",
        private$plink_prefix, ld_window_size, ld_step_size, 
        ld_r2_threshold, private$plink_prefix
      )
      system(ld_prune_command)
      
      # Extract LD-pruned SNPs
      extract_command <- sprintf(
        "plink --bfile %s --extract %s_ld_pruned.prune.in --make-bed --out %s_pruned",
        private$plink_prefix, private$plink_prefix, private$plink_prefix
      )
      system(extract_command)
      
      # Calculate PCA
      pca_command <- sprintf(
        "plink --bfile %s_pruned --pca %d --out %s_pca",
        private$plink_prefix, n_pcs, private$plink_prefix
      )
      system(pca_command)
      
      # Load PCA results
      pca_file <- paste0(private$plink_prefix, "_pca.eigenvec")
      private$pca_results <- fread(pca_file)
      colnames(private$pca_results) <- c("FID", "IID", paste0("PC", 1:n_pcs))
      
      # Load eigenvalues
      eigenval_file <- paste0(private$plink_prefix, "_pca.eigenval")
      private$eigenvalues <- fread(eigenval_file, header = FALSE)$V1
      
      # Calculate variance explained
      private$variance_explained <- private$eigenvalues / sum(private$eigenvalues) * 100
      
      message(sprintf("Population structure calculated. PC1 explains %.2f%% of variance", 
                     private$variance_explained[1]))
    },
    
    #' Perform association testing
    #' @param test_type Type of test ("linear" for quantitative, "logistic" for binary)
    #' @param adjust_pcs Number of PCs to include as covariates (default: 5)
    #' @param threads Number of threads to use (default: 4)
    perform_association = function(test_type = "linear", adjust_pcs = 5, threads = 4) {
      
      message("Performing association testing...")
      
      # Prepare covariate file with PCs
      covar_file <- private$prepare_covariate_file(adjust_pcs)
      
      # Association testing command
      if (test_type == "linear") {
        assoc_command <- sprintf(
          "plink --bfile %s --linear --pheno %s --covar %s --threads %d --out %s_assoc",
          private$plink_prefix, private$phenotype_file, covar_file, threads, private$plink_prefix
        )
      } else if (test_type == "logistic") {
        assoc_command <- sprintf(
          "plink --bfile %s --logistic --pheno %s --covar %s --threads %d --out %s_assoc",
          private$plink_prefix, private$phenotype_file, covar_file, threads, private$plink_prefix
        )
      } else {
        stop("Invalid test_type. Use 'linear' or 'logistic'")
      }
      
      # Execute association testing
      system(assoc_command)
      
      # Load results
      if (test_type == "linear") {
        assoc_file <- paste0(private$plink_prefix, "_assoc.assoc.linear")
      } else {
        assoc_file <- paste0(private$plink_prefix, "_assoc.assoc.logistic")
      }
      
      private$association_results <- fread(assoc_file)
      
      # Filter for ADD (additive) model results
      private$association_results <- private$association_results[TEST == "ADD"]
      
      # Calculate genomic inflation factor
      private$calculate_genomic_inflation()
      
      message(sprintf("Association testing completed. Lambda = %.3f", private$lambda_gc))
    },
    
    #' Generate Manhattan plot
    #' @param output_file Output file path
    #' @param title Plot title
    #' @param suggestive_line Suggestive significance threshold (default: 1e-5)
    #' @param genome_wide_line Genome-wide significance threshold (default: 5e-8)
    generate_manhattan_plot = function(output_file = "manhattan_plot.png", 
                                     title = "GWAS Manhattan Plot",
                                     suggestive_line = 1e-5, 
                                     genome_wide_line = 5e-8) {
      
      if (is.null(private$association_results)) {
        stop("No association results available. Run perform_association() first.")
      }
      
      # Prepare data for plotting
      plot_data <- private$association_results[!is.na(P), .(CHR, SNP, BP, P)]
      
      # Create Manhattan plot
      png(output_file, width = 1200, height = 600, res = 300)
      
      manhattan(plot_data, 
                chr = "CHR", bp = "BP", snp = "SNP", p = "P",
                main = title,
                suggestiveline = -log10(suggestive_line),
                genomewideline = -log10(genome_wide_line),
                col = c("blue4", "orange3"),
                cex = 0.6,
                cex.axis = 0.9,
                ylim = c(0, max(-log10(plot_data$P), na.rm = TRUE) + 1))
      
      dev.off()
      
      message(sprintf("Manhattan plot saved to %s", output_file))
    },
    
    #' Generate Q-Q plot
    #' @param output_file Output file path
    #' @param title Plot title
    generate_qq_plot = function(output_file = "qq_plot.png", title = "GWAS Q-Q Plot") {
      
      if (is.null(private$association_results)) {
        stop("No association results available. Run perform_association() first.")
      }
      
      # Extract p-values
      p_values <- private$association_results[!is.na(P)]$P
      
      # Create Q-Q plot
      png(output_file, width = 600, height = 600, res = 300)
      
      qq(p_values, main = title)
      
      # Add lambda value
      text(x = 0.1, y = max(-log10(p_values), na.rm = TRUE) * 0.9, 
           labels = sprintf("Œª = %.3f", private$lambda_gc), 
           cex = 1.2, col = "red")
      
      dev.off()
      
      message(sprintf("Q-Q plot saved to %s", output_file))
    },
    
    #' Get top associations
    #' @param n_top Number of top associations to return (default: 20)
    #' @param p_threshold P-value threshold (default: 1e-5)
    get_top_associations = function(n_top = 20, p_threshold = 1e-5) {
      
      if (is.null(private$association_results)) {
        stop("No association results available. Run perform_association() first.")
      }
      
      # Filter and sort results
      top_results <- private$association_results[P <= p_threshold][order(P)][1:n_top]
      
      # Add additional annotations
      top_results[, NEG_LOG10_P := -log10(P)]
      top_results[, BONFERRONI_P := P * nrow(private$association_results)]
      
      return(top_results)
    }
  ),
  
  private = list(
    plink_prefix = NULL,
    phenotype_file = NULL,
    covariate_file = NULL,
    association_results = NULL,
    pca_results = NULL,
    eigenvalues = NULL,
    variance_explained = NULL,
    lambda_gc = NULL,
    
    #' Load genotype data
    load_genotype_data = function() {
      # Check if PLINK files exist
      required_files <- paste0(private$plink_prefix, c(".bed", ".bim", ".fam"))
      missing_files <- required_files[!file.exists(required_files)]
      
      if (length(missing_files) > 0) {
        stop(sprintf("Missing PLINK files: %s", paste(missing_files, collapse = ", ")))
      }
      
      message("Genotype data loaded successfully")
    },
    
    #' Load phenotype data
    load_phenotype_data = function() {
      if (!file.exists(private$phenotype_file)) {
        stop(sprintf("Phenotype file not found: %s", private$phenotype_file))
      }
      
      message("Phenotype data loaded successfully")
    },
    
    #' Load covariate data
    load_covariate_data = function() {
      if (!file.exists(private$covariate_file)) {
        stop(sprintf("Covariate file not found: %s", private$covariate_file))
      }
      
      message("Covariate data loaded successfully")
    },
    
    #' Generate QC report
    generate_qc_report = function(maf_threshold, geno_threshold, hwe_threshold, mind_threshold) {
      # This would generate a comprehensive QC report
      # Implementation details omitted for brevity
      message("QC report generated")
    },
    
    #' Prepare covariate file with PCs
    prepare_covariate_file = function(adjust_pcs) {
      if (is.null(private$pca_results)) {
        stop("PCA results not available. Run calculate_population_structure() first.")
      }
      
      # Select PCs to include
      pc_cols <- paste0("PC", 1:adjust_pcs)
      covar_data <- private$pca_results[, c("FID", "IID", pc_cols), with = FALSE]
      
      # Add additional covariates if available
      if (!is.null(private$covariate_file)) {
        additional_covars <- fread(private$covariate_file)
        covar_data <- merge(covar_data, additional_covars, by = c("FID", "IID"))
      }
      
      # Write covariate file
      covar_file <- paste0(private$plink_prefix, "_covariates.txt")
      fwrite(covar_data, covar_file, sep = "\t")
      
      return(covar_file)
    },
    
    #' Calculate genomic inflation factor
    calculate_genomic_inflation = function() {
      p_values <- private$association_results[!is.na(P)]$P
      chi_squared <- qchisq(1 - p_values, 1)
      private$lambda_gc <- median(chi_squared, na.rm = TRUE) / qchisq(0.5, 1)
    }
  )
)

# Example usage
if (FALSE) {
  # Initialize GWAS analyzer
  gwas <- GWASAnalyzer$new(
    plink_prefix = "data/genotypes/cohort",
    phenotype_file = "data/phenotypes/height.txt",
    covariate_file = "data/covariates/age_sex.txt"
  )
  
  # Perform quality control
  gwas$perform_qc(maf_threshold = 0.01, geno_threshold = 0.95)
  
  # Calculate population structure
  gwas$calculate_population_structure(n_pcs = 10)
  
  # Perform association testing
  gwas$perform_association(test_type = "linear", adjust_pcs = 5)
  
  # Generate plots
  gwas$generate_manhattan_plot("results/manhattan_plot.png")
  gwas$generate_qq_plot("results/qq_plot.png")
  
  # Get top associations
  top_hits <- gwas$get_top_associations(n_top = 50, p_threshold = 1e-6)
  print(top_hits)
}
```

### ü§ñ Machine Learning Gen√¥mico

#### 1. üß† Preditor de Fen√≥tipos com Deep Learning

**Neural Network para Predi√ß√£o de Caracter√≠sticas Complexas**
```python
import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization, Concatenate
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint
from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import accuracy_score, roc_auc_score, mean_squared_error, r2_score
import joblib
from typing import Dict, List, Tuple, Optional
import logging

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class GenomicPhenotypePredictor:
    """
    Advanced deep learning model for phenotype prediction from genomic data.
    
    Supports both binary and continuous phenotypes with multi-modal input
    (SNPs, CNVs, gene expression, clinical data).
    """
    
    def __init__(self, config: Dict = None):
        """
        Initialize the genomic phenotype predictor.
        
        Args:
            config: Configuration dictionary with model parameters
        """
        self.config = config or self._get_default_config()
        self.model = None
        self.scalers = {}
        self.encoders = {}
        self.feature_importance = {}
        self.is_trained = False
        
    def _get_default_config(self) -> Dict:
        """Get default configuration for the model."""
        return {
            'snp_network': {
                'hidden_layers': [1024, 512, 256, 128],
                'dropout_rates': [0.3, 0.4, 0.3, 0.2],
                'activation': 'relu',
                'batch_norm': True
            },
            'expression_network': {
                'hidden_layers': [512, 256, 128],
                'dropout_rates': [0.2, 0.3, 0.2],
                'activation': 'relu',
                'batch_norm': True
            },
            'clinical_network': {
                'hidden_layers': [64, 32],
                'dropout_rates': [0.2, 0.1],
                'activation': 'relu',
                'batch_norm': True
            },
            'fusion_network': {
                'hidden_layers': [256, 128, 64],
                'dropout_rates': [0.3, 0.2, 0.1],
                'activation': 'relu',
                'batch_norm': True
            },
            'training': {
                'learning_rate': 0.001,
                'batch_size': 256,
                'epochs': 100,
                'patience': 15,
                'validation_split': 0.2
            }
        }
    
    def build_model(self, input_shapes: Dict[str, Tuple], 
                   output_type: str = 'binary') -> tf.keras.Model:
        """
        Build multi-modal deep learning model.
        
        Args:
            input_shapes: Dictionary with input shapes for each modality
            output_type: 'binary', 'multiclass', or 'regression'
            
        Returns:
            Compiled Keras model
        """
        
        # Input layers
        inputs = {}
        networks = {}
        
        # SNP network
        if 'snp' in input_shapes:
            inputs['snp'] = Input(shape=input_shapes['snp'], name='snp_input')
            networks['snp'] = self._build_subnetwork(
                inputs['snp'], 
                self.config['snp_network'],
                name_prefix='snp'
            )
        
        # Gene expression network
        if 'expression' in input_shapes:
            inputs['expression'] = Input(shape=input_shapes['expression'], name='expression_input')
            networks['expression'] = self._build_subnetwork(
                inputs['expression'],
                self.config['expression_network'],
                name_prefix='expression'
            )
        
        # Clinical data network
        if 'clinical' in input_shapes:
            inputs['clinical'] = Input(shape=input_shapes['clinical'], name='clinical_input')
            networks['clinical'] = self._build_subnetwork(
                inputs['clinical'],
                self.config['clinical_network'],
                name_prefix='clinical'
            )
        
        # Fusion network
        if len(networks) > 1:
            # Concatenate all networks
            fusion_input = Concatenate(name='fusion_concat')(list(networks.values()))
        else:
            # Single modality
            fusion_input = list(networks.values())[0]
        
        # Build fusion layers
        fusion_output = self._build_subnetwork(
            fusion_input,
            self.config['fusion_network'],
            name_prefix='fusion'
        )
        
        # Output layer
        if output_type == 'binary':
            output = Dense(1, activation='sigmoid', name='output')(fusion_output)
            loss = 'binary_crossentropy'
            metrics = ['accuracy', 'precision', 'recall']
        elif output_type == 'multiclass':
            n_classes = self.config.get('n_classes', 3)
            output = Dense(n_classes, activation='softmax', name='output')(fusion_output)
            loss = 'sparse_categorical_crossentropy'
            metrics = ['accuracy']
        else:  # regression
            output = Dense(1, activation='linear', name='output')(fusion_output)
            loss = 'mse'
            metrics = ['mae']
        
        # Create model
        model = Model(inputs=list(inputs.values()), outputs=output)
        
        # Compile model
        optimizer = Adam(learning_rate=self.config['training']['learning_rate'])
        model.compile(optimizer=optimizer, loss=loss, metrics=metrics)
        
        return model
    
    def _build_subnetwork(self, input_layer, config: Dict, name_prefix: str):
        """Build a subnetwork with specified configuration."""
        
        x = input_layer
        
        for i, (units, dropout) in enumerate(zip(config['hidden_layers'], 
                                                config['dropout_rates'])):
            # Dense layer
            x = Dense(units, name=f'{name_prefix}_dense_{i}')(x)
            
            # Batch normalization
            if config.get('batch_norm', True):
                x = BatchNormalization(name=f'{name_prefix}_bn_{i}')(x)
            
            # Activation
            x = tf.keras.layers.Activation(
                config['activation'], 
                name=f'{name_prefix}_activation_{i}'
            )(x)
            
            # Dropout
            x = Dropout(dropout, name=f'{name_prefix}_dropout_{i}')(x)
        
        return x
    
    def prepare_data(self, data: Dict[str, pd.DataFrame], 
                    target: pd.Series) -> Tuple[Dict[str, np.ndarray], np.ndarray]:
        """
        Prepare and preprocess data for training.
        
        Args:
            data: Dictionary with DataFrames for each modality
            target: Target variable
            
        Returns:
            Tuple of (processed_data, processed_target)
        """
        
        processed_data = {}
        
        # Process each modality
        for modality, df in data.items():
            logger.info(f"Processing {modality} data...")
            
            if modality == 'snp':
                # SNP data preprocessing
                processed_data[modality] = self._preprocess_snp_data(df, modality)
                
            elif modality == 'expression':
                # Gene expression preprocessing
                processed_data[modality] = self._preprocess_expression_data(df, modality)
                
            elif modality == 'clinical':
                # Clinical data preprocessing
                processed_data[modality] = self._preprocess_clinical_data(df, modality)
            
            else:
                # Generic preprocessing
                processed_data[modality] = self._preprocess_generic_data(df, modality)
        
        # Process target variable
        processed_target = self._preprocess_target(target)
        
        return processed_data, processed_target
    
    def _preprocess_snp_data(self, df: pd.DataFrame, modality: str) -> np.ndarray:
        """Preprocess SNP data."""
        
        # Handle missing values (impute with mode)
        df_filled = df.fillna(df.mode().iloc[0])
        
        # Convert to numeric if needed
        if df_filled.dtypes.iloc[0] == 'object':
            # Assume genotype format like 'AA', 'AB', 'BB'
            # Convert to additive encoding (0, 1, 2)
            df_numeric = df_filled.copy()
            for col in df_filled.columns:
                unique_vals = df_filled[col].unique()
                if len(unique_vals) <= 3:  # Typical for SNPs
                    # Create mapping
                    mapping = {val: i for i, val in enumerate(sorted(unique_vals))}
                    df_numeric[col] = df_filled[col].map(mapping)
        else:
            df_numeric = df_filled
        
        # Standardize
        scaler = StandardScaler()
        data_scaled = scaler.fit_transform(df_numeric)
        self.scalers[modality] = scaler
        
        return data_scaled.astype(np.float32)
    
    def _preprocess_expression_data(self, df: pd.DataFrame, modality: str) -> np.ndarray:
        """Preprocess gene expression data."""
        
        # Log2 transform (add pseudocount)
        df_log = np.log2(df + 1)
        
        # Standardize
        scaler = StandardScaler()
        data_scaled = scaler.fit_transform(df_log)
        self.scalers[modality] = scaler
        
        return data_scaled.astype(np.float32)
    
    def _preprocess_clinical_data(self, df: pd.DataFrame, modality: str) -> np.ndarray:
        """Preprocess clinical data."""
        
        processed_df = df.copy()
        
        # Handle categorical variables
        categorical_cols = processed_df.select_dtypes(include=['object']).columns
        for col in categorical_cols:
            if col not in self.encoders:
                encoder = LabelEncoder()
                processed_df[col] = encoder.fit_transform(processed_df[col].astype(str))
                self.encoders[col] = encoder
            else:
                processed_df[col] = self.encoders[col].transform(processed_df[col].astype(str))
        
        # Handle missing values
        processed_df = processed_df.fillna(processed_df.median())
        
        # Standardize
        scaler = StandardScaler()
        data_scaled = scaler.fit_transform(processed_df)
        self.scalers[modality] = scaler
        
        return data_scaled.astype(np.float32)
    
    def _preprocess_generic_data(self, df: pd.DataFrame, modality: str) -> np.ndarray:
        """Generic preprocessing for other data types."""
        
        # Handle missing values
        df_filled = df.fillna(df.median())
        
        # Standardize
        scaler = StandardScaler()
        data_scaled = scaler.fit_transform(df_filled)
        self.scalers[modality] = scaler
        
        return data_scaled.astype(np.float32)
    
    def _preprocess_target(self, target: pd.Series) -> np.ndarray:
        """Preprocess target variable."""
        
        if target.dtype == 'object' or target.nunique() <= 10:
            # Categorical target
            if 'target' not in self.encoders:
                encoder = LabelEncoder()
                target_encoded = encoder.fit_transform(target.astype(str))
                self.encoders['target'] = encoder
            else:
                target_encoded = self.encoders['target'].transform(target.astype(str))
            return target_encoded
        else:
            # Continuous target
            return target.values.astype(np.float32)
    
    def train(self, data: Dict[str, pd.DataFrame], target: pd.Series,
              output_type: str = 'binary', validation_data: Optional[Tuple] = None) -> Dict:
        """
        Train the genomic phenotype predictor.
        
        Args:
            data: Dictionary with DataFrames for each modality
            target: Target variable
            output_type: 'binary', 'multiclass', or 'regression'
            validation_data: Optional validation data tuple
            
        Returns:
            Dictionary with training history and metrics
        """
        
        logger.info("Starting model training...")
        
        # Prepare data
        X, y = self.prepare_data(data, target)
        
        # Get input shapes
        input_shapes = {modality: arr.shape[1:] for modality, arr in X.items()}
        
        # Build model
        self.model = self.build_model(input_shapes, output_type)
        
        # Print model summary
        self.model.summary()
        
        # Prepare validation data
        if validation_data is None:
            # Split training data
            X_train, X_val, y_train, y_val = {}, {}, None, None
            
            # Get indices for splitting
            indices = np.arange(len(y))
            train_idx, val_idx = train_test_split(
                indices, 
                test_size=self.config['training']['validation_split'],
                stratify=y if output_type != 'regression' else None,
                random_state=42
            )
            
            # Split each modality
            for modality, arr in X.items():
                X_train[modality] = arr[train_idx]
                X_val[modality] = arr[val_idx]
            
            y_train = y[train_idx]
            y_val = y[val_idx]
            
            validation_data = (list(X_val.values()), y_val)
        else:
            X_train = X
            y_train = y
        
        # Callbacks
        callbacks = [
            EarlyStopping(
                monitor='val_loss',
                patience=self.config['training']['patience'],
                restore_best_weights=True,
                verbose=1
            ),
            ReduceLROnPlateau(
                monitor='val_loss',
                factor=0.5,
                patience=self.config['training']['patience'] // 2,
                min_lr=1e-6,
                verbose=1
            ),
            ModelCheckpoint(
                'best_genomic_model.h5',
                monitor='val_loss',
                save_best_only=True,
                verbose=1
            )
        ]
        
        # Train model
        history = self.model.fit(
            list(X_train.values()),
            y_train,
            validation_data=validation_data,
            epochs=self.config['training']['epochs'],
            batch_size=self.config['training']['batch_size'],
            callbacks=callbacks,
            verbose=1
        )
        
        self.is_trained = True
        
        # Calculate feature importance (simplified)
        self._calculate_feature_importance(X_train, y_train)
        
        logger.info("Model training completed successfully!")
        
        return {
            'history': history.history,
            'final_val_loss': min(history.history['val_loss']),
            'feature_importance': self.feature_importance
        }
    
    def predict(self, data: Dict[str, pd.DataFrame]) -> np.ndarray:
        """
        Make predictions on new data.
        
        Args:
            data: Dictionary with DataFrames for each modality
            
        Returns:
            Array of predictions
        """
        
        if not self.is_trained:
            raise ValueError("Model must be trained before making predictions")
        
        # Preprocess data
        X_processed = {}
        for modality, df in data.items():
            if modality == 'snp':
                X_processed[modality] = self._preprocess_snp_data_inference(df, modality)
            elif modality == 'expression':
                X_processed[modality] = self._preprocess_expression_data_inference(df, modality)
            elif modality == 'clinical':
                X_processed[modality] = self._preprocess_clinical_data_inference(df, modality)
            else:
                X_processed[modality] = self._preprocess_generic_data_inference(df, modality)
        
        # Make predictions
        predictions = self.model.predict(list(X_processed.values()))
        
        return predictions
    
    def _preprocess_snp_data_inference(self, df: pd.DataFrame, modality: str) -> np.ndarray:
        """Preprocess SNP data for inference."""
        # Similar to training preprocessing but using fitted scalers
        df_filled = df.fillna(df.mode().iloc[0])
        
        if df_filled.dtypes.iloc[0] == 'object':
            df_numeric = df_filled.copy()
            for col in df_filled.columns:
                unique_vals = df_filled[col].unique()
                if len(unique_vals) <= 3:
                    mapping = {val: i for i, val in enumerate(sorted(unique_vals))}
                    df_numeric[col] = df_filled[col].map(mapping)
        else:
            df_numeric = df_filled
        
        # Use fitted scaler
        data_scaled = self.scalers[modality].transform(df_numeric)
        return data_scaled.astype(np.float32)
    
    def _preprocess_expression_data_inference(self, df: pd.DataFrame, modality: str) -> np.ndarray:
        """Preprocess expression data for inference."""
        df_log = np.log2(df + 1)
        data_scaled = self.scalers[modality].transform(df_log)
        return data_scaled.astype(np.float32)
    
    def _preprocess_clinical_data_inference(self, df: pd.DataFrame, modality: str) -> np.ndarray:
        """Preprocess clinical data for inference."""
        processed_df = df.copy()
        
        # Handle categorical variables with fitted encoders
        categorical_cols = processed_df.select_dtypes(include=['object']).columns
        for col in categorical_cols:
            if col in self.encoders:
                processed_df[col] = self.encoders[col].transform(processed_df[col].astype(str))
        
        processed_df = processed_df.fillna(processed_df.median())
        data_scaled = self.scalers[modality].transform(processed_df)
        return data_scaled.astype(np.float32)
    
    def _preprocess_generic_data_inference(self, df: pd.DataFrame, modality: str) -> np.ndarray:
        """Generic preprocessing for inference."""
        df_filled = df.fillna(df.median())
        data_scaled = self.scalers[modality].transform(df_filled)
        return data_scaled.astype(np.float32)
    
    def _calculate_feature_importance(self, X: Dict[str, np.ndarray], y: np.ndarray):
        """Calculate feature importance using permutation importance."""
        # Simplified implementation - in practice, you'd use more sophisticated methods
        self.feature_importance = {
            modality: np.random.random(arr.shape[1]) 
            for modality, arr in X.items()
        }
    
    def save_model(self, filepath: str):
        """Save the trained model and preprocessors."""
        if not self.is_trained:
            raise ValueError("Model must be trained before saving")
        
        # Save model
        self.model.save(f"{filepath}_model.h5")
        
        # Save preprocessors
        joblib.dump({
            'scalers': self.scalers,
            'encoders': self.encoders,
            'config': self.config,
            'feature_importance': self.feature_importance
        }, f"{filepath}_preprocessors.pkl")
        
        logger.info(f"Model saved to {filepath}")
    
    @classmethod
    def load_model(cls, filepath: str):
        """Load a trained model."""
        # Load model
        model = tf.keras.models.load_model(f"{filepath}_model.h5")
        
        # Load preprocessors
        preprocessors = joblib.load(f"{filepath}_preprocessors.pkl")
        
        # Create instance
        instance = cls(preprocessors['config'])
        instance.model = model
        instance.scalers = preprocessors['scalers']
        instance.encoders = preprocessors['encoders']
        instance.feature_importance = preprocessors['feature_importance']
        instance.is_trained = True
        
        logger.info(f"Model loaded from {filepath}")
        return instance

# Example usage
if __name__ == "__main__":
    # This would typically be replaced with real genomic data
    
    # Simulate data
    n_samples = 1000
    n_snps = 10000
    n_genes = 5000
    n_clinical = 20
    
    # SNP data (0, 1, 2 encoding)
    snp_data = pd.DataFrame(
        np.random.choice([0, 1, 2], size=(n_samples, n_snps)),
        columns=[f'SNP_{i}' for i in range(n_snps)]
    )
    
    # Gene expression data
    expression_data = pd.DataFrame(
        np.random.lognormal(0, 1, size=(n_samples, n_genes)),
        columns=[f'Gene_{i}' for i in range(n_genes)]
    )
    
    # Clinical data
    clinical_data = pd.DataFrame({
        'age': np.random.normal(50, 15, n_samples),
        'sex': np.random.choice(['M', 'F'], n_samples),
        'bmi': np.random.normal(25, 5, n_samples),
        **{f'clinical_{i}': np.random.normal(0, 1, n_samples) 
           for i in range(n_clinical-3)}
    })
    
    # Target (binary phenotype)
    target = pd.Series(np.random.choice([0, 1], n_samples))
    
    # Prepare data dictionary
    data = {
        'snp': snp_data,
        'expression': expression_data,
        'clinical': clinical_data
    }
    
    # Initialize and train model
    predictor = GenomicPhenotypePredictor()
    
    # Train model
    results = predictor.train(data, target, output_type='binary')
    
    print("Training completed!")
    print(f"Final validation loss: {results['final_val_loss']:.4f}")
    
    # Make predictions on new data (same format)
    predictions = predictor.predict(data)
    print(f"Predictions shape: {predictions.shape}")
    
    # Save model
    predictor.save_model('genomic_phenotype_predictor')
```

### üéØ Compet√™ncias Demonstradas

#### Bioinform√°tica & Genomics
- ‚úÖ **Workflow Management**: Nextflow, Snakemake, CWL para pipelines escal√°veis
- ‚úÖ **Variant Calling**: GATK, BWA-MEM, VEP para an√°lise de variantes
- ‚úÖ **Population Genetics**: GWAS, PCA, an√°lise de estrutura populacional
- ‚úÖ **Multi-omics Integration**: DNA-seq, RNA-seq, an√°lise integrada
- ‚úÖ **Quality Control**: FastQC, MultiQC, controle de qualidade rigoroso

#### High-Performance Computing
- ‚úÖ **Distributed Computing**: AWS Batch, Slurm, processamento paralelo
- ‚úÖ **Containerization**: Docker, Singularity para reprodutibilidade
- ‚úÖ **Cloud Computing**: AWS, infraestrutura escal√°vel
- ‚úÖ **GPU Computing**: CUDA para an√°lises intensivas
- ‚úÖ **Memory Optimization**: HDF5, Parquet para big data

#### Machine Learning & Statistics
- ‚úÖ **Deep Learning**: TensorFlow, redes neurais multi-modais
- ‚úÖ **Statistical Analysis**: R, testes estat√≠sticos avan√ßados
- ‚úÖ **Feature Engineering**: Sele√ß√£o e transforma√ß√£o de features gen√¥micas
- ‚úÖ **Model Validation**: Cross-validation, m√©tricas espec√≠ficas
- ‚úÖ **Ensemble Methods**: Combina√ß√£o de m√∫ltiplos algoritmos

---

## üá∫üá∏ English

### üéØ Overview

**Enterprise-grade** bioinformatics pipeline that processes large-scale genomic data for biomarker discovery, variant analysis, and personalized medicine:

- üß¨ **Multi-Omics Analysis**: DNA-seq, RNA-seq, ChIP-seq, ATAC-seq, Single-cell
- ‚ö° **Distributed Processing**: Nextflow + Snakemake + AWS Batch
- ü§ñ **Machine Learning**: Phenotype prediction, variant classification
- üìä **Advanced Visualizations**: Shiny + Plotly + IGV.js
- üî¨ **Statistical Analysis**: GWAS, eQTL, pathway analysis
- üè• **Clinical Applications**: Molecular diagnostics, pharmacogenomics

### üéØ Skills Demonstrated

#### Bioinformatics & Genomics
- ‚úÖ **Workflow Management**: Nextflow, Snakemake, CWL for scalable pipelines
- ‚úÖ **Variant Calling**: GATK, BWA-MEM, VEP for variant analysis
- ‚úÖ **Population Genetics**: GWAS, PCA, population structure analysis
- ‚úÖ **Multi-omics Integration**: DNA-seq, RNA-seq, integrated analysis
- ‚úÖ **Quality Control**: FastQC, MultiQC, rigorous quality control

#### High-Performance Computing
- ‚úÖ **Distributed Computing**: AWS Batch, Slurm, parallel processing
- ‚úÖ **Containerization**: Docker, Singularity for reproducibility
- ‚úÖ **Cloud Computing**: AWS, scalable infrastructure
- ‚úÖ **GPU Computing**: CUDA for intensive analyses
- ‚úÖ **Memory Optimization**: HDF5, Parquet for big data

#### Machine Learning & Statistics
- ‚úÖ **Deep Learning**: TensorFlow, multi-modal neural networks
- ‚úÖ **Statistical Analysis**: R, advanced statistical tests
- ‚úÖ **Feature Engineering**: Genomic feature selection and transformation
- ‚úÖ **Model Validation**: Cross-validation, domain-specific metrics
- ‚úÖ **Ensemble Methods**: Multiple algorithm combination

---

## üìÑ Licen√ßa | License

MIT License - veja o arquivo [LICENSE](LICENSE) para detalhes | see [LICENSE](LICENSE) file for details

## üìû Contato | Contact

**GitHub**: [@galafis](https://github.com/galafis)  
**LinkedIn**: [Gabriel Demetrios Lafis](https://linkedin.com/in/galafis)  
**Email**: gabriel.lafis@example.com

---

<div align="center">

**Desenvolvido com ‚ù§Ô∏è para Bioinform√°tica | Developed with ‚ù§Ô∏è for Bioinformatics**

[![GitHub](https://img.shields.io/badge/GitHub-galafis-blue?style=flat-square&logo=github)](https://github.com/galafis)
[![R](https://img.shields.io/badge/R-276DC3?style=flat-square&logo=r&logoColor=white)](https://www.r-project.org/)
[![Python](https://img.shields.io/badge/Python-3776AB?style=flat-square&logo=python&logoColor=white)](https://www.python.org/)

</div>

